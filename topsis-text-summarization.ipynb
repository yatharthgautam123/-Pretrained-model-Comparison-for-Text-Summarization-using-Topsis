{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7504305,"sourceType":"datasetVersion","datasetId":4370096}],"dockerImageVersionId":30635,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Importing Dependencies and Dataset**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\nfrom nltk.translate.bleu_score import corpus_bleu\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom math import exp\nfrom tqdm import tqdm \nimport numpy as np \nimport pandas as pd \nimport os\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-29T09:38:31.822387Z","iopub.execute_input":"2024-01-29T09:38:31.823033Z","iopub.status.idle":"2024-01-29T09:38:49.554758Z","shell.execute_reply.started":"2024-01-29T09:38:31.823001Z","shell.execute_reply":"2024-01-29T09:38:49.553687Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"/kaggle/input/mydata-csv/train.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Lading data and selecting pretranied models**","metadata":{}},{"cell_type":"code","source":"# Load Data\ntrain_df_ = pd.read_csv(\"/kaggle/input/mydata-csv/train.csv\")\ntrain_df = train_df_.head(150)\n\n# Select Pretrained Models\nmodels = [\n    \"facebook/bart-large-cnn\",\n    \"t5-large\",\n    \"sshleifer/distilbart-cnn-12-6\",\n    \"google/pegasus-large\",\n    \"allenai/led-large-16384-arxiv\",\n]","metadata":{"execution":{"iopub.status.busy":"2024-01-29T09:38:49.556717Z","iopub.execute_input":"2024-01-29T09:38:49.557269Z","iopub.status.idle":"2024-01-29T09:38:49.796110Z","shell.execute_reply.started":"2024-01-29T09:38:49.557242Z","shell.execute_reply":"2024-01-29T09:38:49.795318Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"**Calculating BLEU Score, Semantic Coherence, Factual Accuracy and Content Coverage to compare the above 5 models**","metadata":{}},{"cell_type":"code","source":"# Initialize BLEU Scores list\nbleu_scores = []\n\n# Semantic Coherence (Example implementation)\ndef semantic_coherence(generated_summary, dialogue):\n    # Your semantic coherence metric calculation logic\n    summary_tokens = word_tokenize(generated_summary.lower())\n    dialogue_tokens = word_tokenize(dialogue.lower())\n\n    # Calculate the intersection of tokens\n    common_tokens = set(summary_tokens) & set(dialogue_tokens)\n\n    # Calculate semantic coherence score based on the ratio of common tokens to summary length\n    coherence_score = len(common_tokens) / len(summary_tokens)\n\n    return coherence_score\n\n# Factual Accuracy (Example implementation)\ndef factual_accuracy(generated_summary, reference_summary):\n    # Your factual accuracy metric calculation logic\n    gen_tokens = set(word_tokenize(generated_summary.lower()))\n    ref_tokens = set(word_tokenize(reference_summary.lower()))\n\n    # Calculate the intersection of tokens\n    common_tokens = gen_tokens & ref_tokens\n\n    # Calculate factual accuracy score based on the ratio of common tokens to reference summary length\n    accuracy_score = len(common_tokens) / len(ref_tokens) if len(ref_tokens) != 0 else 0\n    return accuracy_score\n\n# Content Coverage (Example implementation)\ndef content_coverage(generated_summary, dialogue):\n    # Your content coverage metric calculation logic\n    \n    summary_tokens = set(word_tokenize(generated_summary.lower()))\n    dialogue_tokens = set(word_tokenize(dialogue.lower()))\n\n    # Calculate the intersection of tokens\n    common_tokens = summary_tokens & dialogue_tokens\n\n    # Calculate the content coverage score based on the ratio of common tokens to dialogue length\n    coverage_score = len(common_tokens) / len(dialogue_tokens) if len(dialogue_tokens) != 0 else 0\n    return coverage_score\n\n# Initialize evaluation results DataFrame\nevaluation_results = pd.DataFrame(columns=[\"Model\", \"BLEU Score\", \"Semantic Coherence\", \"Factual Accuracy\", \"Content Coverage\"])\n\n# Initialize empty list to store evaluation results\nevaluation_results_list = []\n\n# Apply Models and Evaluate\nfor model_name in models:\n    print(f\"Evaluating model: {model_name}\")\n    \n    # Initialize the summarization pipeline\n    summarizer = pipeline(\"summarization\", model=model_name, tokenizer=model_name)\n    \n    # Initialize evaluation metric accumulators\n    semantic_coherence_scores = []\n    factual_accuracy_scores = []\n    content_coverage_scores = []\n    \n    # Generate summaries\n    generated_summaries = []\n    for index, row in tqdm(train_df.iterrows(), total=len(train_df)):  # Use tqdm to show progress\n        dialogue = row['dialogue']\n        summary = row['summary']\n        generated_summary = summarizer(dialogue, max_length=100, min_length=30, do_sample=False)[0][\"summary_text\"]\n        generated_summaries.append(generated_summary)\n        \n        # Evaluate Semantic Coherence\n        coherence_score = semantic_coherence(generated_summary, dialogue)\n        semantic_coherence_scores.append(coherence_score)\n        \n        # Evaluate Factual Accuracy\n        accuracy_score = factual_accuracy(generated_summary, summary)\n        factual_accuracy_scores.append(accuracy_score)\n        \n        # Evaluate Content Coverage\n        coverage_score = content_coverage(generated_summary, dialogue)\n        content_coverage_scores.append(coverage_score)\n\n    \n    # Calculate BLEU Score\n    reference_summaries = train_df[\"summary\"].tolist()\n    bleu_score = corpus_bleu([[summary] for summary in reference_summaries], generated_summaries)\n    bleu_scores.append(bleu_score)\n    \n    # Append results to the evaluation results list\n    evaluation_results_list.append({\n        \"Model\": model_name,\n        \"BLEU Score\": bleu_score,\n        \"Semantic Coherence\": sum(semantic_coherence_scores) / len(semantic_coherence_scores),\n        \"Factual Accuracy\": sum(factual_accuracy_scores) / len(factual_accuracy_scores),\n        \"Content Coverage\": sum(content_coverage_scores) / len(content_coverage_scores),\n    })\n    \n    # Print a separator for clarity\n    print(\"=\"*50)\n\n# Concatenate the evaluation results list into a DataFrame\nevaluation_results = pd.concat([pd.DataFrame(item, index=[0]) for item in evaluation_results_list], ignore_index=True)\n\n# Compare Results\nprint(\"BLEU Scores:\", bleu_scores)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-29T09:38:49.797263Z","iopub.execute_input":"2024-01-29T09:38:49.797533Z","iopub.status.idle":"2024-01-29T11:57:03.138460Z","shell.execute_reply.started":"2024-01-29T09:38:49.797508Z","shell.execute_reply":"2024-01-29T11:57:03.137563Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Evaluating model: facebook/bart-large-cnn\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46ae8fd98c64a51b4285bd16f761e42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a759cfe42a1249f698fdae2c12483b6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"65ae4b2fa053442eac77df73b140fde1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c40ed68a80754ac7876017073a183dcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1785e6105dd48fb832c2155455a216c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2311bad7b7014d89801bf282d1063454"}},"metadata":{}},{"name":"stderr","text":"  4%|▍         | 6/150 [00:32<12:59,  5.42s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n  8%|▊         | 12/150 [01:02<11:34,  5.03s/it]Your max_length is set to 100, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n 10%|█         | 15/150 [01:20<12:55,  5.75s/it]Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 11%|█         | 16/150 [01:25<12:14,  5.48s/it]Your max_length is set to 100, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n 21%|██▏       | 32/150 [02:51<09:51,  5.01s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 34%|███▍      | 51/150 [04:48<08:51,  5.37s/it]Your max_length is set to 100, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 35%|███▌      | 53/150 [04:59<08:35,  5.31s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 37%|███▋      | 56/150 [05:17<09:06,  5.82s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 51%|█████▏    | 77/150 [07:16<06:34,  5.40s/it]Your max_length is set to 100, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n 52%|█████▏    | 78/150 [07:21<06:17,  5.24s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 55%|█████▍    | 82/150 [07:44<06:36,  5.84s/it]Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n 59%|█████▊    | 88/150 [08:23<06:34,  6.37s/it]Your max_length is set to 100, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n 73%|███████▎  | 110/150 [10:27<03:49,  5.73s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 81%|████████  | 121/150 [11:25<02:25,  5.02s/it]Your max_length is set to 100, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n 86%|████████▌ | 129/150 [12:11<02:06,  6.04s/it]Your max_length is set to 100, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n 87%|████████▋ | 130/150 [12:18<02:04,  6.23s/it]Your max_length is set to 100, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n100%|██████████| 150/150 [14:20<00:00,  5.74s/it]\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nEvaluating model: t5-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f3e32cf1cd4a548bc9ecb0772de39b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73cc098f05934e04b7a438da896b9222"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2e35c0cd3eb4b97bca34088094c84c3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aaee678579a6423698e9b9c89f4c3399"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38e0d5493014954bea9ae2ff35fd222"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\nFor now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n- Be aware that you SHOULD NOT rely on t5-large automatically truncating your input to 512 when padding/encoding.\n- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n  warnings.warn(\n  4%|▍         | 6/150 [01:25<32:21, 13.48s/it]Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n 10%|█         | 15/150 [03:12<28:15, 12.56s/it]Your max_length is set to 100, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 11%|█         | 16/150 [03:21<25:42, 11.51s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 21%|██▏       | 32/150 [06:54<22:27, 11.42s/it]Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n 34%|███▍      | 51/150 [11:28<22:29, 13.63s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 35%|███▌      | 53/150 [11:58<23:32, 14.56s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 37%|███▋      | 56/150 [12:34<20:44, 13.24s/it]Your max_length is set to 100, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 39%|███▊      | 58/150 [12:56<18:58, 12.38s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (519 > 512). Running this sequence through the model will result in indexing errors\n 51%|█████▏    | 77/150 [17:17<16:40, 13.71s/it]Your max_length is set to 100, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n 52%|█████▏    | 78/150 [17:25<14:25, 12.02s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 55%|█████▍    | 82/150 [18:15<14:29, 12.78s/it]Your max_length is set to 100, but your input_length is only 91. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n 59%|█████▊    | 88/150 [19:26<12:09, 11.76s/it]Your max_length is set to 100, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n 73%|███████▎  | 110/150 [24:18<08:59, 13.49s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 86%|████████▌ | 129/150 [28:31<05:17, 15.13s/it]Your max_length is set to 100, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n 87%|████████▋ | 130/150 [28:40<04:25, 13.27s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n100%|██████████| 150/150 [33:04<00:00, 13.23s/it]\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nEvaluating model: sshleifer/distilbart-cnn-12-6\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.80k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eec075793c354808990919410b4193da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9536519b4d5466d84cf78821c49970a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c532d4443664293b50fb6254b4c574f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e483a71449154797aae481d0e91f8d34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c596d5497044ce1813ea16f07839ad9"}},"metadata":{}},{"name":"stderr","text":"  4%|▍         | 6/150 [00:25<09:54,  4.13s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n  8%|▊         | 12/150 [00:46<08:39,  3.76s/it]Your max_length is set to 100, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n 10%|█         | 15/150 [01:01<09:27,  4.20s/it]Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 11%|█         | 16/150 [01:05<09:29,  4.25s/it]Your max_length is set to 100, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n 21%|██▏       | 32/150 [02:09<06:54,  3.51s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 34%|███▍      | 51/150 [03:44<07:24,  4.49s/it]Your max_length is set to 100, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 35%|███▌      | 53/150 [03:52<06:47,  4.20s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 37%|███▋      | 56/150 [04:07<07:08,  4.55s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 51%|█████▏    | 77/150 [05:38<05:07,  4.21s/it]Your max_length is set to 100, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n 52%|█████▏    | 78/150 [05:42<05:06,  4.25s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 55%|█████▍    | 82/150 [05:58<04:30,  3.97s/it]Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n 59%|█████▊    | 88/150 [06:28<05:01,  4.87s/it]Your max_length is set to 100, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n 73%|███████▎  | 110/150 [08:03<02:40,  4.00s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 81%|████████  | 121/150 [08:43<01:47,  3.71s/it]Your max_length is set to 100, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n 86%|████████▌ | 129/150 [09:16<01:28,  4.20s/it]Your max_length is set to 100, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n 87%|████████▋ | 130/150 [09:20<01:21,  4.09s/it]Your max_length is set to 100, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n100%|██████████| 150/150 [10:50<00:00,  4.34s/it]\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nEvaluating model: google/pegasus-large\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee4bc64c2a7d4eb7842cd494b3f325b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"192460210579401b92c8ae7bcf04de5a"}},"metadata":{}},{"name":"stderr","text":"Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/260 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04aa463f00c6468c981df5dfe3f9bdc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"633cebeec7ac42c0a6d3c3a640047947"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19bc6cf493724a5bbe1b08f825dce094"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d675b1c2cf5c4623bfab58f7bd866318"}},"metadata":{}},{"name":"stderr","text":"  3%|▎         | 5/150 [01:20<36:38, 15.16s/it]Your max_length is set to 100, but your input_length is only 97. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n  4%|▍         | 6/150 [01:33<34:30, 14.38s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n  8%|▊         | 12/150 [03:16<37:31, 16.31s/it]Your max_length is set to 100, but your input_length is only 97. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n 10%|█         | 15/150 [04:09<40:32, 18.02s/it]Your max_length is set to 100, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n 11%|█         | 16/150 [04:22<37:11, 16.65s/it]Your max_length is set to 100, but your input_length is only 79. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n 21%|██▏       | 32/150 [08:32<29:28, 14.98s/it]Your max_length is set to 100, but your input_length is only 86. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 34%|███▍      | 51/150 [14:10<27:19, 16.56s/it]Your max_length is set to 100, but your input_length is only 80. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n 35%|███▌      | 53/150 [14:39<24:55, 15.42s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 37%|███▋      | 56/150 [15:39<29:51, 19.06s/it]Your max_length is set to 100, but your input_length is only 75. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 51%|█████▏    | 77/150 [21:55<25:18, 20.79s/it]Your max_length is set to 100, but your input_length is only 65. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n 52%|█████▏    | 78/150 [22:18<26:01, 21.68s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 55%|█████▍    | 82/150 [23:38<24:04, 21.24s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 59%|█████▊    | 88/150 [25:14<16:24, 15.88s/it]Your max_length is set to 100, but your input_length is only 90. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=45)\n 73%|███████▎  | 110/150 [31:18<10:53, 16.33s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 81%|████████  | 121/150 [34:29<09:35, 19.85s/it]Your max_length is set to 100, but your input_length is only 88. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n 86%|████████▌ | 129/150 [36:42<05:54, 16.86s/it]Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 87%|████████▋ | 130/150 [36:53<05:01, 15.09s/it]Your max_length is set to 100, but your input_length is only 72. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=36)\n100%|██████████| 150/150 [42:12<00:00, 16.88s/it]\n","output_type":"stream"},{"name":"stdout","text":"==================================================\nEvaluating model: allenai/led-large-16384-arxiv\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e38800ba4cb942489b7eb552a846cc80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.84G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf1e2f07ee9740e18f44030649e76eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a07a2f4ed15c4f049e3d1c9bc94d7ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"367d0bfc5fb34c9d9ba8dd8508e27415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5332ccdbaee46c4a894d004da7b88ea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"784ec45942074a9797ceb39fc50d62e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f109e942293143c0a8335733412a8668"}},"metadata":{}},{"name":"stderr","text":"  4%|▍         | 6/150 [01:28<34:53, 14.54s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n  8%|▊         | 12/150 [02:57<34:01, 14.80s/it]Your max_length is set to 100, but your input_length is only 99. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=49)\n 10%|█         | 15/150 [03:41<33:29, 14.88s/it]Your max_length is set to 100, but your input_length is only 76. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 11%|█         | 16/150 [03:56<32:59, 14.77s/it]Your max_length is set to 100, but your input_length is only 81. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=40)\n 21%|██▏       | 32/150 [07:51<28:49, 14.66s/it]Your max_length is set to 100, but your input_length is only 87. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=43)\n 34%|███▍      | 51/150 [12:33<25:04, 15.20s/it]Your max_length is set to 100, but your input_length is only 83. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 35%|███▌      | 53/150 [13:03<24:28, 15.14s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 37%|███▋      | 56/150 [13:48<23:41, 15.13s/it]Your max_length is set to 100, but your input_length is only 74. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=37)\n 51%|█████▏    | 77/150 [19:00<18:03, 14.84s/it]Your max_length is set to 100, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n 52%|█████▏    | 78/150 [19:14<17:36, 14.67s/it]Your max_length is set to 100, but your input_length is only 77. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=38)\n 55%|█████▍    | 82/150 [20:13<16:40, 14.71s/it]Your max_length is set to 100, but your input_length is only 85. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n 59%|█████▊    | 88/150 [21:41<15:11, 14.69s/it]Your max_length is set to 100, but your input_length is only 96. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n 73%|███████▎  | 110/150 [27:05<09:48, 14.71s/it]Your max_length is set to 100, but your input_length is only 82. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=41)\n 81%|████████  | 121/150 [29:47<07:08, 14.78s/it]Your max_length is set to 100, but your input_length is only 89. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=44)\n 86%|████████▌ | 129/150 [31:44<05:08, 14.70s/it]Your max_length is set to 100, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n 87%|████████▋ | 130/150 [31:59<04:53, 14.66s/it]Your max_length is set to 100, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n100%|██████████| 150/150 [36:52<00:00, 14.75s/it]","output_type":"stream"},{"name":"stdout","text":"==================================================\nBLEU Scores: [0.3488121983156154, 0.3397876196809291, 0.30525945503925905, 0.36171531600300605, 0.15297269719096934]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Saving results to csv, which will be used as input for calculating topsis score for each model**\n","metadata":{}},{"cell_type":"code","source":"# Save evaluation results to a CSV file\nevaluation_results.to_csv(\"inputfortopsis.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-29T11:57:17.430280Z","iopub.execute_input":"2024-01-29T11:57:17.431144Z","iopub.status.idle":"2024-01-29T11:57:17.436520Z","shell.execute_reply.started":"2024-01-29T11:57:17.431111Z","shell.execute_reply":"2024-01-29T11:57:17.435600Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"evaluation_results.head()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"                           Model  BLEU Score  Semantic Coherence  \\\n0        facebook/bart-large-cnn    0.348812            0.655736   \n1                       t5-large    0.339788            0.755254   \n2  sshleifer/distilbart-cnn-12-6    0.305259            0.708327   \n3           google/pegasus-large    0.361715            0.707104   \n4  allenai/led-large-16384-arxiv    0.152973            0.243868   \n\n   Factual Accuracy  Content Coverage  \n0          0.458349          0.371619  \n1          0.457265          0.408908  \n2          0.521721          0.483919  \n3          0.420562          0.363004  \n4          0.284099          0.186362  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>BLEU Score</th>\n      <th>Semantic Coherence</th>\n      <th>Factual Accuracy</th>\n      <th>Content Coverage</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>facebook/bart-large-cnn</td>\n      <td>0.348812</td>\n      <td>0.655736</td>\n      <td>0.458349</td>\n      <td>0.371619</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>t5-large</td>\n      <td>0.339788</td>\n      <td>0.755254</td>\n      <td>0.457265</td>\n      <td>0.408908</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sshleifer/distilbart-cnn-12-6</td>\n      <td>0.305259</td>\n      <td>0.708327</td>\n      <td>0.521721</td>\n      <td>0.483919</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>google/pegasus-large</td>\n      <td>0.361715</td>\n      <td>0.707104</td>\n      <td>0.420562</td>\n      <td>0.363004</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>allenai/led-large-16384-arxiv</td>\n      <td>0.152973</td>\n      <td>0.243868</td>\n      <td>0.284099</td>\n      <td>0.186362</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}